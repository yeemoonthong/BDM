<a href="https://github.com/drshahizan/BDM/stargazers"><img src="https://img.shields.io/github/stars/drshahizan/BDM" alt="Stars Badge"/></a>
<a href="https://github.com/drshahizan/BDM/network/members"><img src="https://img.shields.io/github/forks/drshahizan/BDM" alt="Forks Badge"/></a>
<a href="https://github.com/drshahizan/BDM/pulls"><img src="https://img.shields.io/github/issues-pr/drshahizan/BDM" alt="Pull Requests Badge"/></a>
<a href="https://github.com/drshahizan/BDM"><img src="https://img.shields.io/github/issues/drshahizan/BDM" alt="Issues Badge"/></a>
<a href="https://github.com/drshahizan/BDM/graphs/contributors"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/drshahizan/BDM?color=2b9348"></a>
![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan%2BDM&labelColor=%23d9e3f0&countColor=%23697689&style=flat)



# How to Become a Data Engineer in 2024

<p align="center">
<img src="../images/roadmap_de.gif"  height="700" />
</p>

## Step 1: Learn a programming language

Data engineers need to know how to code in at least one programming language, such as Python, Java, or Scala. These languages are used to create and maintain data pipelines, build data models, and do other data engineering tasks. You can learn these languages through online courses, books, or tutorials.

## Step 2: Learn SQL

SQL is the standard language for working with data in relational databases. Data engineers need to know how to use SQL to query and manipulate data in data pipelines, data warehouses, and other data storage systems. You can learn SQL through online courses, books, or tutorials.

## Step 3: Learn data warehousing and big data technologies

Data engineers need to know how to use data warehousing and big data technologies to create and maintain data pipelines and data warehouses. Data warehousing is the process of collecting, storing, and analyzing data from different sources. Big data technologies are tools and frameworks that enable processing and analyzing large and complex data sets. Some of the common data warehousing and big data technologies include:

- Apache Hadoop: a distributed system that allows storing and processing large data sets across multiple nodes.
- Apache Spark: a fast and general-purpose engine for large-scale data processing.
- Apache Kafka: a distributed streaming platform that allows publishing and subscribing to streams of data.
- Apache Hive: a data warehouse software that allows reading, writing, and managing large data sets using SQL.
- Apache Cassandra: a distributed database that provides high availability and scalability.

You can learn these technologies through online courses, books, or tutorials.

## Step 4: Learn cloud computing platforms

Cloud computing platforms such as Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure are widely used by data engineers to create and manage data pipelines and data warehouses. These platforms offer various services and tools that simplify data engineering tasks, such as:

- AWS S3: a scalable and durable object storage service that can store any type of data.
- AWS Lambda: a serverless computing service that allows running code without provisioning or managing servers.
- GCP BigQuery: a serverless and scalable data warehouse that can run fast and complex queries on large data sets.
- GCP Cloud Functions: a serverless computing service that allows running code in response to events.
- Azure Blob Storage: a scalable and durable object storage service that can store any type of data.
- Azure Functions: a serverless computing service that allows running code without provisioning or managing servers.

It is important for data engineers to know how to use these platforms. You can learn these platforms through online courses, books, or tutorials.

## Step 5: Gain experience with data engineering tools

There are many data engineering tools that can be used to create and manage data pipelines, data warehouses, and other data engineering tasks. Some popular data engineering tools include:

- Apache Airflow: a platform that allows creating, scheduling, and monitoring workflows of data pipelines.
- AWS Glue: a serverless data integration service that allows creating, running, and monitoring data pipelines.
- Google Cloud Dataflow: a service that allows creating and running data pipelines for batch and stream processing.
- Azure Data Factory: a service that allows creating and running data pipelines for batch and stream processing.

You can gain experience with these tools by using them in your projects or by following online tutorials.

## Step 6: Build a portfolio of projects

Once you have a good understanding of the basic skills and tools needed for data engineering, it is important to start building a portfolio of projects. This will help you to show your skills to potential employers. You can find many project ideas and tutorials online. You can also contribute to open source data engineering projects. Some examples of data engineering projects are:

- Creating a data pipeline that collects, cleans, and transforms data from different sources and loads it into a data warehouse.
- Creating a data warehouse that stores and organizes data from different sources and enables fast and complex queries.
- Creating a data lake that stores raw and unstructured data from different sources and enables flexible and scalable data processing.
- Creating a data dashboard that visualizes and analyzes data from different sources and provides insights and recommendations.

## Step 7: Network with other data engineers

Networking with other data engineers is a great way to learn about new technologies and trends, and to find job opportunities. You can network with other data engineers online through social media, forums, and meetups. You can also attend data engineering conferences and workshops. Some examples of data engineering communities and events are:

- Data Engineering Podcast: a podcast that covers the latest trends and topics in data engineering.
- Data Engineering Meetup: a meetup group that organizes events and talks on data engineering.
- Data Engineering Summit: a conference that brings together data engineers and data scientists to share best practices and innovations in data engineering.


## Contribution üõ†Ô∏è
Please create an [Issue](https://github.com/drshahizan/BDM/issues) for any improvements, suggestions or errors in the content.

You can also contact me using [Linkedin](https://www.linkedin.com/in/drshahizan/) for any other queries or feedback.

[![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan&labelColor=%23697689&countColor=%23555555&style=plastic)](https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fdrshahizan)
![](https://hit.yhype.me/github/profile?user_id=81284918)
